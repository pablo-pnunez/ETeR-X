{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Obtain from each model:** Results, execution time, parameters and CO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "from src.experiments.Common import load_best_model\n",
    "from matplotlib.ticker import FixedLocator\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "baselines_path = \"models/Baselines\"\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "our_model=\"ETeR-X\"\n",
    "\n",
    "set_names = {\"Amazon\":\"AM\", \"Pois\":\"POI\", \"Restaurants\":\"RST\"}\n",
    "subset_names = {\"Digital_Music\":\"MSC\", \"Music\":\"MSC\", \"Fashion\":\"FSH\", \"Gijon\":\"GJN\", \"Gijón\":\"GJN\", \"Barcelona\":\"BCN\", \"Madrid\":\"MDR\", \"Newyorkcity\":\"NYC\", \"New York\":\"NYC\", \"Paris\":\"PRS\", \"London\":\"LND\"}\n",
    "\n",
    "models_cold = {\"MOSTPOP2ITM\":\"MostPop\", \"BOW2ITM\":our_model, \"USEM2ITM\":\"USEM\", \"BERT2ITM\":\"BERT\"}\n",
    "models_baseline = {\"MOSTPOP2ITM\":\"MostPop\", 'EASEᴿ':\"EASEᴿ\", 'BiVAECF':\"BiVAE\", \"BOW2ITM\":our_model, \"USEM2ITM\":\"USEM\", \"BERT2ITM\":\"BERT\"}\n",
    "\n",
    "model_colors = {'BiVAE':\"#FF708F\", 'EASEᴿ':\"#95ABB2\", 'MostPop':'#AD858B', our_model:'#4CAF50', 'USEM':'#03A9F4', 'BERT':'#ffa726'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_baseline_data(path, dataset, subset):\n",
    "    \n",
    "    def process_section(lines):\n",
    "        # Obtener las columnas\n",
    "        columns = lines[1].split(\"|\")\n",
    "        columns = [col.strip() for col in columns]\n",
    "        \n",
    "        # Obtener los datos\n",
    "        data = []\n",
    "        for line in lines[3:]:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            row = line.split(\"|\")\n",
    "            row = [item.strip() for item in row]\n",
    "            data.append(row)\n",
    "        \n",
    "        # Crear el DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        return df\n",
    "\n",
    "    # Obtener el fichero con los tiempos\n",
    "    paths = sorted(Path(path).iterdir(), key=os.path.getmtime)\n",
    "    filepath = [str(f) for f in paths if \"CornacExp\" in f.name][-1]\n",
    "\n",
    "    # Leer el archivo\n",
    "    with open(filepath, 'r') as file: lines = file.readlines()\n",
    "\n",
    "    # Separar las secciones\n",
    "    validation_start = lines.index('VALIDATION:\\n')\n",
    "    test_start = lines.index('TEST:\\n')\n",
    "\n",
    "    validation_lines = lines[validation_start + 1:test_start]\n",
    "    test_lines = lines[test_start + 1:]\n",
    "\n",
    "    # Procesar cada sección\n",
    "    validation_df = process_section(validation_lines)\n",
    "    test_df = process_section(test_lines)\n",
    "    \n",
    "    test_df.rename(columns={\"\":\"Model\", \"Train (s)\":\"Train_time\"}, inplace=True)\n",
    "    \n",
    "    model_paper_names = {\"GridSearch_EASEᴿ\":\"EASEᴿ\", \"GridSearch_BPR\":\"BPR\", \"online_ibpr\": \"IBPR\"}\n",
    "    \n",
    "    # Obtener los resultados en test de todos los modelos \n",
    "    all_test_results = pd.read_csv(path+\"results.csv\").reset_index(drop=True)\n",
    "    all_test_results = all_test_results.merge(test_df[[\"Model\", \"Train_time\"]], on=\"Model\", how=\"left\")\n",
    "    all_test_results[\"Model\"] = all_test_results[\"Model\"].apply(lambda x: model_paper_names[x] if x in model_paper_names.keys() else x)\n",
    "    \n",
    "    all_test_results.insert(0, \"Subset\", subset)\n",
    "    all_test_results.insert(0, \"Set\", dataset)\n",
    "    return all_test_results\n",
    "\n",
    "def evaluate_cold_start(model_class):\n",
    "    # Eliminar usuarios que aparecen en TRAIN\n",
    "    train_dev_users = model_class.DATASET.DATA[\"TRAIN_DEV\"].userId.unique()\n",
    "    # original_test_len = len(model_class.DATASET.DATA[\"TEST\"])\n",
    "    model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"][~model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)]\n",
    "    model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"].drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False)\n",
    "    # Evaluar en el conjunto de test\n",
    "    test_ret = model_class.train(dev=False, save_model=True)\n",
    "    # Evaluar en el conjunto de test\n",
    "    test_ret = model_class.evaluate(test=True, verbose=False)\n",
    "    return test_ret.to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain number of test users in all/known/cold scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " users_result = []\n",
    " for dataset, subsets in datasets.items():\n",
    "        for subset in subsets:\n",
    "            # Definir el nombre del fichero\n",
    "            base_path = \"/media/nas/pperez/code/2024/ETeR-X/\"\n",
    "            path = f\"{base_path}{baselines_path}/{dataset}/{subset}/\"\n",
    "            # Cargar el primer modelo (todos dan lo mismo)\n",
    "            model_class = load_best_model(model=list(models_cold.keys())[0], dataset=dataset, subset=subset, base_path=base_path)\n",
    "            # Obtener número de usuarios de test\n",
    "            test_total_users = len(model_class.DATASET.DATA[\"TEST\"][\"userId\"].unique())\n",
    "            # Obtener número de usuarios cold-star de test\n",
    "            train_dev_users = model_class.DATASET.DATA[\"TRAIN_DEV\"][\"userId\"].unique()\n",
    "            test_cold_users = len(model_class.DATASET.DATA[\"TEST\"][~model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)][\"userId\"].unique())\n",
    "            # Obtener número de usuarios de test ya vistos en train-dev\n",
    "            test_known_users = len(model_class.DATASET.DATA[\"TEST\"][model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)][\"userId\"].unique())\n",
    "            # Imprimir información\n",
    "            users_result.append((set_names[dataset.title()], subset_names[subset.title()], test_total_users, test_cold_users, test_known_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_user_info = pd.DataFrame(users_result, columns=[\"Set\", \"Subset\", \"All\", \"Cold\", \"Known\"])\n",
    "experiment_user_info.pivot_table(index=[\"Set\", \"Subset\"], values=[\"All\", \"Known\", \"Cold\"]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models/all_baseline_data.xlsx\") and not os.path.exists(\"models/all_cold_start_data.xlsx\"):\n",
    "\n",
    "    all_baseline_data = []\n",
    "    all_cold_start_data = []\n",
    "\n",
    "    for dataset, subsets in datasets.items():\n",
    "        for subset in subsets:\n",
    "            # Definir el nombre del fichero\n",
    "            base_path = \"/media/nas/pperez/code/2024/ETeR-X/\"\n",
    "            path = f\"{base_path}{baselines_path}/{dataset}/{subset}/\"\n",
    "            # Extraer todos los datos de los baselines\n",
    "            baseline_data = extract_baseline_data(path, dataset=set_names[dataset.title()], subset=subset_names[subset.title()])\n",
    "            # Cambiamos los nombres\n",
    "            baseline_data[\"Model\"] = baseline_data[\"Model\"].apply(lambda x: models_baseline[x] if x in models_baseline.keys() else x)\n",
    "            # Eliminamos modelos irrelevantes \n",
    "            baseline_data = baseline_data[baseline_data[\"Model\"].isin(list(models_baseline.values()))]\n",
    "            # Para los modelos no baseline, hacer lo mismo\n",
    "            cold_start_data = []\n",
    "            for model in list(models_cold.keys()):\n",
    "                # Cargar mejor modelo\n",
    "                model_class = load_best_model(model=model, dataset=dataset, subset=subset, base_path=base_path)\n",
    "                # Obtener los resultados en cold_start\n",
    "                metrics = evaluate_cold_start(model_class)\n",
    "                # Obtener el número de parámetros\n",
    "                params = model_class.MODEL.count_params()\n",
    "                # Obtener el tiempo de train\n",
    "                log_data = pd.read_csv(model_class.MODEL_PATH+\"log.csv\")\n",
    "                time = log_data[\"e_time\"].sum()\n",
    "                # Obtener epochs\n",
    "                epochs = log_data[\"epoch\"].max()+1\n",
    "                # Obtener emisiones\n",
    "                emissions = pd.read_csv(model_class.MODEL_PATH+\"emissions_train.csv\").drop_duplicates(\"project_name\")\n",
    "                emissions = emissions[\"emissions\"].sum()\n",
    "                # Añadir a baselines\n",
    "                baseline_data.loc[baseline_data[\"Model\"]==models_cold[model], \"Epochs\"] = epochs\n",
    "                baseline_data.loc[baseline_data[\"Model\"]==models_cold[model], \"Train_time\"] = time\n",
    "                baseline_data.loc[baseline_data[\"Model\"]==models_cold[model], \"Params\"] = params\n",
    "                baseline_data.loc[baseline_data[\"Model\"]==models_cold[model], \"co2\"] = emissions\n",
    "                # Añadir a cold_start\n",
    "                cold_start_data.append({\"Set\":set_names[dataset.title()], \"Subset\":subset_names[subset.title()], \"Model\": models_cold[model], \"Train_time\": time, \"Params\":params, \"Epochs\":epochs, \"co2\": emissions, **metrics})\n",
    "                # Liberar RAM\n",
    "                del model_class\n",
    "                \n",
    "            # Añadir a los correspondientes datasets finales\n",
    "            all_baseline_data.extend(baseline_data.to_dict(orient='records'))\n",
    "            all_cold_start_data.extend(cold_start_data)\n",
    "\n",
    "    all_baseline_data = pd.DataFrame(all_baseline_data)\n",
    "    all_cold_start_data = pd.DataFrame(all_cold_start_data)\n",
    "\n",
    "    # Guardamos los resultados dado que es temporalmente costoso recalcularlos\n",
    "    all_baseline_data.to_excel(\"models/all_baseline_data.xlsx\", index=False)\n",
    "    all_cold_start_data.to_excel(\"models/all_cold_start_data.xlsx\", index=False)\n",
    "\n",
    "else:\n",
    "    # Guardamos los resultados dado que es temporalmente costoso recalcularlos\n",
    "    all_baseline_data = pd.read_excel(\"models/all_baseline_data.xlsx\")\n",
    "    all_cold_start_data = pd.read_excel(\"models/all_cold_start_data.xlsx\")\n",
    "    \n",
    "# Determinamos el orden deseado\n",
    "all_baseline_data[\"Set\"] = pd.Categorical(all_baseline_data[\"Set\"], list(set_names.values()))\n",
    "all_cold_start_data[\"Set\"] = pd.Categorical(all_cold_start_data[\"Set\"], list(set_names.values()))\n",
    "\n",
    "all_baseline_data[\"Subset\"] = pd.Categorical(all_baseline_data[\"Subset\"], list(dict.fromkeys(subset_names.values())))\n",
    "all_cold_start_data[\"Subset\"] = pd.Categorical(all_cold_start_data[\"Subset\"], list(dict.fromkeys(subset_names.values())))\n",
    "\n",
    "all_baseline_data[\"Model\"] = pd.Categorical(all_baseline_data[\"Model\"], list(models_baseline.values()))\n",
    "all_cold_start_data[\"Model\"] = pd.Categorical(all_cold_start_data[\"Model\"], list(models_cold.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_results = all_baseline_data.melt(id_vars=['Set', 'Subset', 'Model'], value_vars=['NDCG@10', 'Recall@10'], var_name='Metric', value_name='Value')\n",
    "bl_results = bl_results.pivot_table(index=['Set', 'Subset', 'Metric'], columns='Model', values='Value')\n",
    "bl_results.to_excel(\"output/baselines_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_results = all_cold_start_data.melt(id_vars=['Set', 'Subset', 'Model'], value_vars=['NDCG@10', 'Recall@10'], var_name='Metric', value_name='Value')\n",
    "cs_results = cs_results.pivot_table(index=['Set', 'Subset', 'Metric'], columns='Model', values='Value')\n",
    "cs_results.to_excel(\"output/cold_start_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables with unique parameter information and times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_results = all_baseline_data[all_baseline_data['Train_time'].notna()].pivot_table(index=[\"Set\", \"Subset\"], columns=[\"Model\"], values=[\"Train_time\"])\n",
    "param_results = all_cold_start_data.pivot_table(index=[\"Set\", \"Subset\"], columns=[\"Model\"], values=[\"Params\"])\n",
    "\n",
    "time_results.to_excel(\"output/time_results.xlsx\")\n",
    "param_results.to_excel(\"output/param_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate graphics for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_comparison(data, column_name=\"co2\", title=\"Total train emissions (gr)\", share_y=False, ylim=None, legend=True, y_scale=\"linear\"):\n",
    "    # Ordenar por modelos\n",
    "    data = data.sort_values([\"Set\", \"Subset\", \"Model\"])\n",
    "    # Calcular el número de elementos en X para cada dataset\n",
    "    num_elements = data[[\"Set\", \"Subset\"]].drop_duplicates().groupby(\"Set\", sort=False)[\"Subset\"].size().values.tolist()\n",
    "    sets = data[\"Set\"].unique()\n",
    "    models = data[\"Model\"].unique()\n",
    "\n",
    "    # Normalizar los anchos de los subplots para que el total sea 1\n",
    "    total_elements = sum(num_elements)\n",
    "    relative_widths = [n / total_elements for n in num_elements]\n",
    "\n",
    "    # Crear una figura con subplots horizontales y anchos personalizados\n",
    "    fig, axes = plt.subplots(1, len(sets), figsize=(sum(num_elements) * .65+2, 2.8), gridspec_kw={'width_ratios': relative_widths}, sharey=share_y)\n",
    "\n",
    "    # Iterar sobre los datasets y sus correspondientes ejes\n",
    "    for plot_idx, (dataset, ax) in enumerate(zip(sets, axes)):\n",
    "        subset_data = data[data[\"Set\"]==dataset].sort_values(\"Subset\")\n",
    "        X = subset_data[\"Subset\"].unique().tolist()\n",
    "        model_values = subset_data.groupby(\"Model\", sort=False).apply(lambda x: x[column_name].values.tolist()).to_frame().to_dict()[0]\n",
    "\n",
    "        X_axis = np.arange(len(X))\n",
    "        \n",
    "        bar_width = 0.7 / len(model_values)  # Ajustar el ancho de las barras\n",
    "        offset = 0  # Inicializar el desplazamiento\n",
    "        margin = 0.02\n",
    "        # model_names = {\"BOW2ITM\":\"TeRCEx\", \"USEM2ITM\":\"USEM\", \"BERT2ITM\":\"BERT\"}\n",
    "        \n",
    "        for i, (model, values) in enumerate(model_values.items()):\n",
    "            color = model_colors[model] # model_colors[i % len(model_colors)]  # Seleccionar un color de la lista en función de la posición del modelo\n",
    "            ax.bar(X_axis + offset - margin, values, bar_width-margin, label=model, color=color)\n",
    "            offset += bar_width  # Incrementar el desplazamiento para el próximo modelo\n",
    "\n",
    "        ax.set_xticks(X_axis + bar_width * (len(model_values) - 1) / 2)\n",
    "        ax.set_xticklabels(X)  # Ajustar las etiquetas del eje X\n",
    "\n",
    "        # if plot_idx == 0: ax.set_ylabel(axis_label)\n",
    "        if share_y and ylim is not None: ax.set_ylim(ylim)\n",
    "        ax.set_title(dataset)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(axis=\"y\")\n",
    "        ax.set_yscale(y_scale)\n",
    "    \n",
    "    # Mostrar la leyenda solo una vez\n",
    "    if legend:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=len(models))\n",
    "\n",
    "    # Ajustar el layout para que no se solapen los subplots\n",
    "    # plt.title(dataset)\n",
    "    fig.suptitle(title, fontsize=15, fontweight=\"bold\", y=0.95)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"output/bar_plot_{column_name}.pdf\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "bar_plot_comparison(all_baseline_data, column_name=\"Train_time\", y_scale=\"log\", share_y=True, title=\"Training time (s)\")\n",
    "\n",
    "# Eliminamos el MostPop, para estos gráficos, no aportan mucho\n",
    "cold_no_pop = all_cold_start_data.loc[all_cold_start_data[\"Model\"]!=\"MostPop\"].reset_index(drop=True)\n",
    "cold_no_pop[\"Model\"] = pd.Categorical(cold_no_pop[\"Model\"], ['ETeR-X', 'USEM', 'BERT'])\n",
    "cold_no_pop = cold_no_pop.sort_values([\"Set\", \"Subset\"])\n",
    "\n",
    "fig1 = bar_plot_comparison(cold_no_pop, column_name=\"co2\", y_scale=\"log\", share_y=True, title=\"Train emissions (g of CO2)\")\n",
    "fig2 = bar_plot_comparison(cold_no_pop, column_name=\"Params\", y_scale=\"log\", share_y=True, title=\"Model parameters\")\n",
    "fig2 = bar_plot_comparison(cold_no_pop, column_name=\"Epochs\", y_scale=\"log\", share_y=True, title=\"Model epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version of the previous one that allows stacking of several figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_comparison(data, axes, column_name=\"co2\", title=\"Total train emissions (gr)\", share_y=False, ylim=None, legend=True, show_title=True, show_x_labels=True, y_scale=\"linear\"):\n",
    "    # Ordenar por modelos\n",
    "    data = data.sort_values([\"Set\", \"Subset\", \"Model\"])\n",
    "    # Calcular el número de elementos en X para cada dataset\n",
    "    num_elements = data[[\"Set\", \"Subset\"]].drop_duplicates().groupby(\"Set\", sort=False)[\"Subset\"].size().values.tolist()\n",
    "    sets = data[\"Set\"].unique()\n",
    "    models = data[\"Model\"].unique()\n",
    "\n",
    "    # Normalizar los anchos de los subplots para que el total sea 1\n",
    "    total_elements = sum(num_elements)\n",
    "    relative_widths = [n / total_elements for n in num_elements]\n",
    "\n",
    "    # Iterar sobre los datasets y sus correspondientes ejes\n",
    "    for plot_idx, (dataset, ax) in enumerate(zip(sets, axes.flatten())):\n",
    "        X = data[data[\"Set\"]==dataset][\"Subset\"].unique().tolist()\n",
    "        model_values = data[data[\"Set\"]==dataset].groupby(\"Model\", sort=False).apply(lambda x: x[column_name].values.tolist()).to_frame().to_dict()[0]\n",
    "\n",
    "        X_axis = np.arange(len(X))\n",
    "        \n",
    "        bar_width = 0.7 / len(model_values)  # Ajustar el ancho de las barras\n",
    "        offset = 0  # Inicializar el desplazamiento\n",
    "        margin = 0.02\n",
    "        \n",
    "        for i, (model, values) in enumerate(model_values.items()):\n",
    "            color = model_colors[model]\n",
    "            try:\n",
    "                ax.bar(X_axis + offset - margin, values, bar_width-margin, label=model, color=color)\n",
    "            except:\n",
    "                print(model, values, color, dataset)\n",
    "            offset += bar_width  # Incrementar el desplazamiento para el próximo modelo\n",
    "\n",
    "        if show_x_labels:\n",
    "            ax.set_xticks(X_axis + bar_width * (len(model_values) - 1) / 2)\n",
    "            ax.set_xticklabels(X)  # Ajustar las etiquetas del eje X\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "        \n",
    "        if plot_idx==0: ax.set_ylabel(title)\n",
    "\n",
    "        if share_y and ylim is not None: ax.set_ylim(ylim)\n",
    "        if show_title: ax.set_title(dataset)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(axis=\"y\")\n",
    "        ax.set_yscale(y_scale)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "data = cold_no_pop\n",
    "num_elements = data[[\"Set\", \"Subset\"]].drop_duplicates().groupby(\"Set\", sort=False)[\"Subset\"].size().values.tolist()\n",
    "sets = data[\"Set\"].unique()\n",
    "models = data[\"Model\"].unique()\n",
    "\n",
    "# Normalizar los anchos de los subplots para que el total sea 1\n",
    "total_elements = sum(num_elements)\n",
    "relative_widths = [n / total_elements for n in num_elements]\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(sum(num_elements) * .6+1, 3.5), gridspec_kw={'width_ratios': relative_widths}, sharey='row')\n",
    "\n",
    "bar_plot_comparison(cold_no_pop, ax[0,:], column_name=\"co2\", y_scale=\"log\", share_y=True, show_x_labels=False, title=\"Emissions\")\n",
    "bar_plot_comparison(cold_no_pop, ax[1,:], column_name=\"Params\", y_scale=\"log\", share_y=True, show_title=False, title=\"Parameters\")\n",
    "\n",
    "handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.025), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1, top=0.85, bottom=0.15)\n",
    "\n",
    "plt.savefig(f\"output/bar_plot_co2_params.pdf\",bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polar_plot_comparison(data, metrics=[\"NDCG@10\", \"Recall@10\"], output_file=\"polar_plot.pdf\"):\n",
    "    \n",
    "    def cerrar_ciclo(data):\n",
    "        return np.append(data, data[0])\n",
    "    \n",
    "    def plot_radial_data(axis, models, datasets, plot_data, column, title=\"Title\", max_y=None, fmt='{:.3f}'):\n",
    "        axis.set_theta_zero_location(\"N\")\n",
    "        axis.set_theta_direction(-1)\n",
    "\n",
    "        for idx, modelo in enumerate(models):\n",
    "            values = plot_data.loc[plot_data[\"Model\"] == modelo][column].values\n",
    "            values = np.append(values, values[0])\n",
    "            theta = np.linspace(0, 2 * np.pi, len(values), endpoint=True)\n",
    "            color = model_colors[modelo]  # Seleccionar un color de la lista en función de la posición del modelo\n",
    "            axis.fill(theta, values, alpha=0.1, color=color)\n",
    "            axis.plot(theta, values, label=modelo, marker=\".\",  color=color)\n",
    "        axis.set_title(title, pad=30, fontweight=\"bold\")\n",
    "        \n",
    "        if max_y is not None: axis.set_ylim(0, max_y)\n",
    "        axis.yaxis.set_major_locator(FixedLocator(axis.get_yticks()))\n",
    "        axis.set_yticklabels([fmt.format(tick) for tick in axis.get_yticks()], ha='center', va='bottom', color='lightgrey', fontweight='bold')\n",
    "        axis.set_axisbelow(True)\n",
    "        # Get radial labels away from plotted line\n",
    "        axis.set_rlabel_position(180)  \n",
    "        axis.set_xticks(np.linspace(0, 2*np.pi, len(datasets), endpoint=False))\n",
    "        axis.set_xticklabels(datasets, fontsize=10)\n",
    "        # Adjust grid colors and style\n",
    "        axis.grid(linewidth=1, linestyle = '-', color=\"lightgrey\")\n",
    "        axis.yaxis.grid(linewidth=0.5, color=\"lightgrey\")\n",
    "        # Adjust datasets labels, rotation, color, ...\n",
    "        angles = np.linspace(0, -2*np.pi, len(axis.get_xticklabels()) + 1)\n",
    "        angles = np.rad2deg(angles[:-1])\n",
    "        labels = []\n",
    "        for label, angle in zip(axis.get_xticklabels(), angles):\n",
    "            x, y = label.get_position()\n",
    "            dataset_text = label.get_text()\n",
    "            dataset_color = \"darkred\" if \"POI\" in dataset_text else \"navy\" if \"RST\" in dataset_text else \"darkslategray\"\n",
    "            # Ajustar la posición de las etiquetas y rotarlas para que formen 90 grados con el radio\n",
    "            if angle <= -90 and angle >= -270: rotation_angle = angle  - 180\n",
    "            else: rotation_angle = angle\n",
    "            lab = axis.text(x, y + 0.035, dataset_text, transform=label.get_transform(), ha='center', va='center', rotation=rotation_angle, color=dataset_color)\n",
    "            labels.append(lab)\n",
    "        axis.set_xticklabels([])\n",
    "\n",
    "    # Lista de modelos\n",
    "    modelos = list(data[\"Model\"].sort_values().unique())\n",
    "    # Crear una lista de sets+subsets como títulos\n",
    "    datasets = data[[\"Set\", \"Subset\"]].drop_duplicates().apply(lambda x: x[\"Set\"]+\"-\"+x[\"Subset\"], axis=1).values.tolist()\n",
    "    # Contar sets y subsets\n",
    "    n_datasets = len(datasets)\n",
    "    # Crear los dos gráficos radiales (polar plots)\n",
    "    fig, axs = plt.subplots(1, 2, subplot_kw=dict(polar=True), figsize=(8,5))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for id_plot, metric in enumerate(metrics):\n",
    "        plot_radial_data(axs[id_plot], modelos, datasets, data, fmt='{:.1f}',max_y=1, column=metric, title=metric)\n",
    "        \n",
    "    # Añadir una sola leyenda en el centro abajo\n",
    "    lines, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(lines, labels, loc='lower center', bbox_to_anchor=(0.5, 0.15), ncol=1)\n",
    "    # Ajustar el espacio horizontal entre los gráficos\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(wspace=0.6, hspace=0, top=0.85, bottom=0.15)\n",
    "    # Guardar figura\n",
    "    plt.savefig(\"output/\"+output_file, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "polar_plot_comparison(all_baseline_data, metrics=[\"NDCG@10\", \"Recall@10\"], output_file=\"baseline_polar_plot.pdf\")\n",
    "polar_plot_comparison(all_cold_start_data, metrics=[\"NDCG@10\", \"Recall@10\"], output_file=\"cold_start_polar_plot.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
